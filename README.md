# Customer Support AI Assistant

## ğŸ” Overview
This project is a Retrieval-Augmented Generation (RAG) based AI assistant designed to help answer customer support queries. It retrieves relevant past Q&A data and generates contextual, human-like responses using the Falcon-7B model from Hugging Face.

---

## âœ… Submission Checklist
- [x] Clean, modular GitHub repository âœ…
- [x] API endpoint (optional: can be deployed using Replit/Render) âœ…
- [x] README and PPT with approach, RAG details, and explainability âœ…

---

## ğŸ› ï¸ Tech Stack
- Python
- FastAPI
- Hugging Face Transformers & Inference API
- FAISS (Facebook AI Similarity Search)
- SentenceTransformers (`all-MiniLM-L6-v2`)

---

## ğŸ§  Approach Taken
1. **Data Loading & Cleaning:**
   - Loaded customer support Q&A dataset from Hugging Face: `MohammadOthman/mo-customer-support-tweets-945k`
   - Cleaned null and duplicate values.
   - Saved as `clean_data.csv`.

2. **Embedding & Indexing:**
   - Used SentenceTransformer model `all-MiniLM-L6-v2` for embedding queries.
   - Built a FAISS vector index to support fast semantic similarity search.

3. **RAG Pipeline:**
   - **Retrieval:** Given a query, retrieve top 3 most semantically similar past Q&As.
   - **Augmentation:** Create a prompt by combining retrieved Q&As with the user's query.
   - **Generation:** Send the prompt to Falcon-7B via Hugging Face API to generate a final response.

4. **Serving via API:**
   - Used FastAPI to expose a `/generate` endpoint for the assistant.

---

## ğŸ”„ RAG Implementation
**Retrieval**:
- Vector representation of queries built using `all-MiniLM-L6-v2`.
- Indexed using FAISS with L2 similarity.
- Search function retrieves top_k matches.

**Augmentation**:
- Selected top 3 matches.
- Created structured prompt with context Q&A + user query.

**Generation**:
- Used `tiiuae/falcon-7b-instruct` via Hugging Face Inference API.
- Output is post-processed to return only the generated answer.

---

## ğŸ“¢ Explainability Techniques
- Matching Q&A pairs and similarity scores are logged and can be optionally returned.
- Keeps responses traceable and easy to debug.
- (Optional future enhancement: use LIME/SHAP for deeper model transparency.)

---

## ğŸ“¦ Directory Structure
```
customer-support-ai/
â”œâ”€â”€ app.py                            # FastAPI app entrypoint
â”œâ”€â”€ pipeline.py                       # RAG pipeline (retriever + generator)
â”œâ”€â”€ requirements.txt                  # Dependencies
â”œâ”€â”€ README.md                         # Project overview & how to run
â”œâ”€â”€ .gitignore                        # Ignore unwanted files (e.g., __pycache__)
â”‚
â”œâ”€â”€ rag/                              # Core logic of RAG
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embedder.py                   # SentenceTransformer embedder
â”‚   â”œâ”€â”€ vector_store.py               # FAISS vector DB logic
â”‚   â”œâ”€â”€ retriever.py                  # Retrieval logic
â”‚   â”œâ”€â”€ generator.py                  # LLM-based response generator
â”‚
â”œâ”€â”€ test/                             # Test scripts for each module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_embedder.py              # (optional, can test embeddings)
â”‚   â”œâ”€â”€ test_vector_store.py
â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â”œâ”€â”€ test_generator.py
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ dataset/                          # Dataset preprocessing & raw/clean files
â”‚   â”œâ”€â”€ dataset_preprocessor.py
â”‚   â”œâ”€â”€ raw/                          # (optional) raw downloaded files
â”‚   â””â”€â”€ storage/                      # Cleaned CSV, FAISS index, mapping.pkl
â”‚       â”œâ”€â”€ clean_data.csv
â”‚       â”œâ”€â”€ faiss_index.index
â”‚       â”œâ”€â”€ mapping.pkl
â”‚
â”œâ”€â”€ logs/                             # Logs for interaction history
â”‚   â””â”€â”€ conversations.csv


```

---

## ğŸš€ How to Run Locally
```bash
# Step 1: Install dependencies
pip install -r requirements.txt

# Step 2: Run FastAPI server
uvicorn app.main:app --reload
```

---

## ğŸ” Notes on Security
- API key is stored in `.env` file and not pushed to GitHub.
- Use `python-dotenv` to load it securely.

---

## ğŸ¤ Credits
Built by Aadil Maqbool for the Customer Support AI Assistant Challenge.

